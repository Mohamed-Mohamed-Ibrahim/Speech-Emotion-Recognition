{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008},{"sourceId":11800768,"sourceType":"datasetVersion","datasetId":7410307}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](attachment:65b9293e-2d96-4789-9bdb-96ad17697d96.png)","metadata":{},"attachments":{"65b9293e-2d96-4789-9bdb-96ad17697d96.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"# Resourses\n- https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d\n- https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition\n- https://librosa.org/doc/latest/index.html\n- https://www.kaggle.com/code/hossamemamo/speech-emotion-recognition-2-parallel-cnn-conv-2d\n- https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition\n- ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Audio\n# from entropy import spectral_entropy\n# from keras import layers\n# from keras import models\n# from keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n# from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport itertools, torch, sys\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:06:10.46723Z","iopub.execute_input":"2025-05-13T20:06:10.467483Z","iopub.status.idle":"2025-05-13T20:06:15.200404Z","shell.execute_reply.started":"2025-05-13T20:06:10.467462Z","shell.execute_reply":"2025-05-13T20:06:15.199842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to\nRavdess = \"../input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24\"\nCrema = \"../input/speech-emotion-recognition-en/Crema\"\nSavee = \"../input/speech-emotion-recognition-en/Savee\"\nTess = \"../input/speech-emotion-recognition-en/Tess\"\nprint(os.listdir(Ravdess))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:39.363293Z","iopub.execute_input":"2025-05-13T20:07:39.364091Z","iopub.status.idle":"2025-05-13T20:07:39.382424Z","shell.execute_reply.started":"2025-05-13T20:07:39.364065Z","shell.execute_reply":"2025-05-13T20:07:39.381842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nemotion_df = []\n\nfor dir in ravdess_directory_list:\n    actor = os.listdir(os.path.join(Ravdess, dir))\n    for wav in actor:\n        info = wav.partition(\".wav\")[0].split(\"-\")\n        emotion = int(info[2])\n        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:39.749457Z","iopub.execute_input":"2025-05-13T20:07:39.749709Z","iopub.status.idle":"2025-05-13T20:07:40.024845Z","shell.execute_reply.started":"2025-05-13T20:07:39.749691Z","shell.execute_reply":"2025-05-13T20:07:40.023979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Ravdess_df = pd.DataFrame.from_dict(emotion_df)\nRavdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:40.288052Z","iopub.execute_input":"2025-05-13T20:07:40.288563Z","iopub.status.idle":"2025-05-13T20:07:40.294039Z","shell.execute_reply.started":"2025-05-13T20:07:40.28854Z","shell.execute_reply":"2025-05-13T20:07:40.293142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:41.76372Z","iopub.execute_input":"2025-05-13T20:07:41.76452Z","iopub.status.idle":"2025-05-13T20:07:41.774346Z","shell.execute_reply.started":"2025-05-13T20:07:41.764491Z","shell.execute_reply":"2025-05-13T20:07:41.77372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_df = []\n\nfor wav in os.listdir(Crema):\n    info = wav.partition(\".wav\")[0].split(\"_\")\n    if info[2] == 'SAD':\n        emotion_df.append((\"sad\", Crema + \"/\" + wav))\n    elif info[2] == 'ANG':\n        emotion_df.append((\"angry\", Crema + \"/\" + wav))\n    elif info[2] == 'DIS':\n        emotion_df.append((\"disgust\", Crema + \"/\" + wav))\n    elif info[2] == 'FEA':\n        emotion_df.append((\"fear\", Crema + \"/\" + wav))\n    elif info[2] == 'HAP':\n        emotion_df.append((\"happy\", Crema + \"/\" + wav))\n    elif info[2] == 'NEU':\n        emotion_df.append((\"neutral\", Crema + \"/\" + wav))\n    else:\n        emotion_df.append((\"unknown\", Crema + \"/\" + wav))\n\n\nCrema_df = pd.DataFrame.from_dict(emotion_df)\nCrema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nCrema_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:42.007155Z","iopub.execute_input":"2025-05-13T20:07:42.007328Z","iopub.status.idle":"2025-05-13T20:07:42.090081Z","shell.execute_reply.started":"2025-05-13T20:07:42.007315Z","shell.execute_reply":"2025-05-13T20:07:42.089534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nemotion_df = []\n\nfor dir in tess_directory_list:\n    for wav in os.listdir(os.path.join(Tess, dir)):\n        info = wav.partition(\".wav\")[0].split(\"_\")\n        emo = info[2]\n        if emo == \"ps\":\n            emotion_df.append((\"surprise\", os.path.join(Tess, dir, wav)))\n        else:\n            emotion_df.append((emo, os.path.join(Tess, dir, wav)))\n\n\nTess_df = pd.DataFrame.from_dict(emotion_df)\nTess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nTess_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:44.085739Z","iopub.execute_input":"2025-05-13T20:07:44.086407Z","iopub.status.idle":"2025-05-13T20:07:44.249744Z","shell.execute_reply.started":"2025-05-13T20:07:44.086374Z","shell.execute_reply":"2025-05-13T20:07:44.249062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"savee_directiory_list = os.listdir(Savee)\n\nemotion_df = []\n\nfor wav in savee_directiory_list:\n    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n    emotion = re.split(r\"[0-9]\", info)[0]\n    if emotion=='a':\n        emotion_df.append((\"angry\", Savee + \"/\" + wav))\n    elif emotion=='d':\n        emotion_df.append((\"disgust\", Savee + \"/\" + wav))\n    elif emotion=='f':\n        emotion_df.append((\"fear\", Savee + \"/\" + wav))\n    elif emotion=='h':\n        emotion_df.append((\"happy\", Savee + \"/\" + wav))\n    elif emotion=='n':\n        emotion_df.append((\"neutral\", Savee + \"/\" + wav))\n    elif emotion=='sa':\n        emotion_df.append((\"sad\", Savee + \"/\" + wav))\n    else:\n        emotion_df.append((\"surprise\", Savee + \"/\" + wav))\n\n\nSavee_df = pd.DataFrame.from_dict(emotion_df)\nSavee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nSavee_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:44.56725Z","iopub.execute_input":"2025-05-13T20:07:44.56768Z","iopub.status.idle":"2025-05-13T20:07:44.607131Z","shell.execute_reply.started":"2025-05-13T20:07:44.567661Z","shell.execute_reply":"2025-05-13T20:07:44.606577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:47.049242Z","iopub.execute_input":"2025-05-13T20:07:47.049502Z","iopub.status.idle":"2025-05-13T20:07:47.055967Z","shell.execute_reply.started":"2025-05-13T20:07:47.049484Z","shell.execute_reply":"2025-05-13T20:07:47.055255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\n\nplt.style.use(\"ggplot\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:53.704677Z","iopub.execute_input":"2025-05-13T20:07:53.705409Z","iopub.status.idle":"2025-05-13T20:07:53.709545Z","shell.execute_reply.started":"2025-05-13T20:07:53.705386Z","shell.execute_reply":"2025-05-13T20:07:53.708846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title(\"Count of emotions:\")\nsns.countplot(x=df[\"Emotion\"])\nsns.despine(top=True, right=True, left=False, bottom=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:54.648134Z","iopub.execute_input":"2025-05-13T20:07:54.648588Z","iopub.status.idle":"2025-05-13T20:07:54.921274Z","shell.execute_reply.started":"2025-05-13T20:07:54.648554Z","shell.execute_reply":"2025-05-13T20:07:54.920454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:57.493324Z","iopub.execute_input":"2025-05-13T20:07:57.493892Z","iopub.status.idle":"2025-05-13T20:07:57.498945Z","shell.execute_reply.started":"2025-05-13T20:07:57.493868Z","shell.execute_reply":"2025-05-13T20:07:57.498223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='fear'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:40:33.202592Z","iopub.execute_input":"2025-05-13T12:40:33.20286Z","iopub.status.idle":"2025-05-13T12:40:45.606956Z","shell.execute_reply.started":"2025-05-13T12:40:33.202841Z","shell.execute_reply":"2025-05-13T12:40:45.606214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='angry'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:40:45.607987Z","iopub.execute_input":"2025-05-13T12:40:45.608372Z","iopub.status.idle":"2025-05-13T12:40:46.636681Z","shell.execute_reply.started":"2025-05-13T12:40:45.608354Z","shell.execute_reply":"2025-05-13T12:40:46.635929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='sad'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:19:43.040382Z","iopub.execute_input":"2025-05-13T05:19:43.040752Z","iopub.status.idle":"2025-05-13T05:19:44.16419Z","shell.execute_reply.started":"2025-05-13T05:19:43.040721Z","shell.execute_reply":"2025-05-13T05:19:44.16314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data augmentation\n\nWe have some ways for data augmentation in sound data:\n\n1. Noise injection\n2. Stretching\n3. Shifting\n4. Pitching","metadata":{}},{"cell_type":"code","source":"def noise(data, random=False, rate=0.035, threshold=0.075):\n    \"\"\"Add some noise to sound sample. Use random if you want to add random noise with some threshold.\n    Or use rate Random=False and rate for always adding fixed noise.\"\"\"\n    if random:\n        rate = np.random.random() * threshold\n    noise_amp = rate*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    \"\"\"Stretching data with some rate.\"\"\"\n    return librosa.effects.time_stretch(data, rate=rate)\n\ndef shift(data, rate=1000):\n    \"\"\"Shifting data with some rate\"\"\"\n    shift_range = int(np.random.uniform(low=-5, high = 5)*rate)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7, random=False):\n    \"\"\"\"Add some pitch to sound sample. Use random if you want to add random pitch with some threshold.\n    Or use pitch_factor Random=False and rate for always adding fixed pitch.\"\"\"\n    if random:\n        pitch_factor=np.random.random() * pitch_factor\n    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:07.85047Z","iopub.execute_input":"2025-05-13T20:11:07.851341Z","iopub.status.idle":"2025-05-13T20:11:07.857325Z","shell.execute_reply.started":"2025-05-13T20:11:07.85131Z","shell.execute_reply":"2025-05-13T20:11:07.856573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:51.867956Z","iopub.execute_input":"2025-05-13T20:11:51.868415Z","iopub.status.idle":"2025-05-13T20:11:51.875119Z","shell.execute_reply.started":"2025-05-13T20:11:51.868392Z","shell.execute_reply":"2025-05-13T20:11:51.874549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = df[df[\"Emotion\"] == \"happy\"][\"Path\"].iloc[0]\ndata, sampling_rate = librosa.load(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:42.676601Z","iopub.execute_input":"2025-05-13T15:43:42.677301Z","iopub.status.idle":"2025-05-13T15:43:54.187374Z","shell.execute_reply.started":"2025-05-13T15:43:42.67728Z","shell.execute_reply":"2025-05-13T15:43:54.186794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveshow(data, sr=sampling_rate)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:20:50.39065Z","iopub.execute_input":"2025-05-13T11:20:50.390939Z","iopub.status.idle":"2025-05-13T11:20:50.825997Z","shell.execute_reply.started":"2025-05-13T11:20:50.390919Z","shell.execute_reply":"2025-05-13T11:20:50.82527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"noised_data = noise(data, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=noised_data, sr=sampling_rate)\nAudio(noised_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:02:04.37181Z","iopub.execute_input":"2025-05-12T18:02:04.372115Z","iopub.status.idle":"2025-05-12T18:02:04.722574Z","shell.execute_reply.started":"2025-05-12T18:02:04.372095Z","shell.execute_reply":"2025-05-12T18:02:04.721932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stretched_data = stretch(data, rate=0.5)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=stretched_data, sr=sampling_rate)\nAudio(stretched_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:02:52.277361Z","iopub.execute_input":"2025-05-12T18:02:52.277646Z","iopub.status.idle":"2025-05-12T18:02:52.974059Z","shell.execute_reply.started":"2025-05-12T18:02:52.27763Z","shell.execute_reply":"2025-05-12T18:02:52.973325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shifted_data = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=shifted_data, sr=sampling_rate)\nAudio(shifted_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:03:11.085446Z","iopub.execute_input":"2025-05-12T18:03:11.08618Z","iopub.status.idle":"2025-05-12T18:03:11.396191Z","shell.execute_reply.started":"2025-05-12T18:03:11.08616Z","shell.execute_reply":"2025-05-12T18:03:11.395498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pitched_data = pitch(data, sampling_rate, pitch_factor=0.5, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=pitched_data, sr=sampling_rate)\nAudio(pitched_data, rate=sampling_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:03:18.032236Z","iopub.execute_input":"2025-05-12T18:03:18.032526Z","iopub.status.idle":"2025-05-12T18:03:18.353313Z","shell.execute_reply.started":"2025-05-12T18:03:18.032499Z","shell.execute_reply":"2025-05-12T18:03:18.352671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For our data augmentation we will use noise and pitch and combination with both of it.","metadata":{}},{"cell_type":"markdown","source":"## Feature extraction\n\n#### There are some features may be useful:","metadata":{}},{"cell_type":"markdown","source":"1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n3. Entropy of Energy :The entropy of sub-frames’ normalized energies. It can be interpreted as a measure of abrupt changes.\n3. Spectral Centroid : The center of gravity of the spectrum.\n4. Spectral Spread : The second central moment of the spectrum.\n5. Spectral Entropy : Entropy of the normalized spectral energies for a set of sub-frames.\n6. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n7. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n8. MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.","metadata":{}},{"cell_type":"code","source":"n_fft = 2048\nhop_length = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:54.834781Z","iopub.execute_input":"2025-05-13T20:11:54.835465Z","iopub.status.idle":"2025-05-13T20:11:54.838658Z","shell.execute_reply.started":"2025-05-13T20:11:54.83544Z","shell.execute_reply":"2025-05-13T20:11:54.837966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunks(data, frame_length, hop_length):\n    for i in range(0, len(data), hop_length):\n        yield data[i:i+frame_length]\n\n# Zero Crossing Rate\ndef zcr(data, frame_length=2048, hop_length=512):\n    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(zcr)\n\n\ndef energy(data, frame_length=2048, hop_length=512):\n    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])\n    return en / frame_length\n\n\ndef rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n\n\ndef entropy_of_energy(data, frame_length=2048, hop_length=512):\n    energies = energy(data, frame_length, hop_length)\n    energies /= np.sum(energies)\n\n    entropy = 0.0\n    entropy -= energies * np.log2(energies)\n    return entropy\n\n\ndef spc(data, sr, frame_length=2048, hop_length=512):\n    spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spectral_centroid)\n\n\n# def spc_entropy(data, sr):\n#     spc_en = spectral_entropy(data, sf=sr, method=\"fft\")\n#     return spc_en\n\ndef spc_flux(data):\n    isSpectrum = data.ndim == 1\n    if isSpectrum:\n        data = np.expand_dims(data, axis=1)\n\n    X = np.c_[data[:, 0], data]\n    af_Delta_X = np.diff(X, 1, axis=1)\n    vsf = np.sqrt((np.power(af_Delta_X, 2).sum(axis=0))) / X.shape[0]\n\n    return np.squeeze(vsf) if isSpectrum else vsf\n\n\ndef spc_rollof(data, sr, frame_length=2048, hop_length=512):\n    spcrollof = librosa.feature.spectral_rolloff(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spcrollof)\n\n\ndef chroma_stft(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sr)\n    return np.squeeze(chroma_stft.T) if not flatten else np.ravel(chroma_stft.T)\n\n\ndef mel_spc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mel = librosa.feature.melspectrogram(y=data, sr=sr)\n    return np.squeeze(mel.T) if not flatten else np.ravel(mel.T)\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:55.105538Z","iopub.execute_input":"2025-05-13T20:11:55.105734Z","iopub.status.idle":"2025-05-13T20:11:55.116973Z","shell.execute_reply.started":"2025-05-13T20:11:55.105719Z","shell.execute_reply":"2025-05-13T20:11:55.116346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = np.array(df[\"Path\"])[658]\ndata, sampling_rate = librosa.load(path, duration=2.5, offset=0.6)\nlen(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:57.248861Z","iopub.execute_input":"2025-05-13T20:11:57.24913Z","iopub.status.idle":"2025-05-13T20:11:57.263185Z","shell.execute_reply.started":"2025-05-13T20:11:57.24911Z","shell.execute_reply":"2025-05-13T20:11:57.262588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"ZCR: \", zcr(data).shape)\nprint(\"Energy: \", energy(data).shape)\nprint(\"Entropy of Energy :\", entropy_of_energy(data).shape)\nprint(\"RMS :\", rmse(data).shape)\nprint(\"Spectral Centroid :\", spc(data, sampling_rate).shape)\n# print(\"Spectral Entropy: \", spc_entropy(data, sampling_rate).shape)\nprint(\"Spectral Flux: \", spc_flux(data).shape)\nprint(\"Spectral Rollof: \", spc_rollof(data, sampling_rate).shape)\nprint(\"Chroma STFT: \", chroma_stft(data, sampling_rate).shape)\nprint(\"MelSpectrogram: \", mel_spc(data, sampling_rate).shape)\nprint(\"MFCC: \", mfcc(data, sampling_rate).shape)\nprint(\"Melspectrogram: \",np.mean(librosa.feature.melspectrogram(y=data, sr=sampling_rate).T, axis=0).shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:58.692748Z","iopub.execute_input":"2025-05-13T20:11:58.693415Z","iopub.status.idle":"2025-05-13T20:11:58.75177Z","shell.execute_reply.started":"2025-05-13T20:11:58.693396Z","shell.execute_reply":"2025-05-13T20:11:58.751221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(data, sr, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        # np.mean(energy(data, frame_length, hop_length),axis=0),\n                        # np.mean(entropy_of_energy(data, frame_length, hop_length), axis=0),\n                        rmse(data, frame_length, hop_length),\n                        # spc(data, sr, frame_length, hop_length),\n                        # spc_entropy(data, sr),\n                        # spc_flux(data),\n                        # spc_rollof(data, sr, frame_length, hop_length),\n                        # chroma_stft(data, sr, frame_length, hop_length),\n                        # mel_spc(data, sr, frame_length, hop_length, flatten=True)\n                        # mfcc(data, sr, frame_length, hop_length)\n                                    ))\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:12:00.281447Z","iopub.execute_input":"2025-05-13T20:12:00.281943Z","iopub.status.idle":"2025-05-13T20:12:00.286046Z","shell.execute_reply.started":"2025-05-13T20:12:00.281921Z","shell.execute_reply":"2025-05-13T20:12:00.285441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_features(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n     # without augmentation\n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n    # data with noise\n    noise_data = noise(data, random=True)\n    res2 = extract_features(noise_data, sample_rate)\n    result = np.vstack((result, res2)) # stacking vertically\n\n    # data with pitching\n    pitched_data = pitch(data, sample_rate, random=True)\n    res3 = extract_features(pitched_data, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    # data with pitching and white_noise\n    new_data = pitch(data, sample_rate, random=True)\n    data_noise_pitch = noise(new_data, random=True)\n    res3 = extract_features(data_noise_pitch, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:12:01.752235Z","iopub.execute_input":"2025-05-13T20:12:01.752479Z","iopub.status.idle":"2025-05-13T20:12:01.757779Z","shell.execute_reply.started":"2025-05-13T20:12:01.75246Z","shell.execute_reply":"2025-05-13T20:12:01.757159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Y = [], []\nprint(\"Feature processing...\")\nfor path, emotion, ind in zip(df.Path, df.Emotion, range(df.Path.shape[0])):\n    features = get_features(path)\n    # print(features.shape)\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n     \n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n    # print(len(X[0]))\n    # print(len(X))\n    # print(len(Y))\n    # break\nprint(\"Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:13:44.53418Z","iopub.execute_input":"2025-05-13T20:13:44.53445Z","iopub.status.idle":"2025-05-13T20:32:15.245035Z","shell.execute_reply.started":"2025-05-13T20:13:44.53443Z","shell.execute_reply":"2025-05-13T20:32:15.244369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's save our features as DataFrame for further processing:","metadata":{}},{"cell_type":"code","source":"features_path = \"./features.csv\"\nfeatures_path_2d = \"./features_2d.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:36:13.762948Z","iopub.execute_input":"2025-05-13T20:36:13.763508Z","iopub.status.idle":"2025-05-13T20:36:13.768192Z","shell.execute_reply.started":"2025-05-13T20:36:13.763486Z","shell.execute_reply":"2025-05-13T20:36:13.767482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df = pd.DataFrame(X)\nextracted_df[\"labels\"] = Y\nextracted_df.to_csv(features_path, index=False)\nextracted_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:33:57.913914Z","iopub.execute_input":"2025-05-13T20:33:57.914321Z","iopub.status.idle":"2025-05-13T20:34:11.0648Z","shell.execute_reply.started":"2025-05-13T20:33:57.914304Z","shell.execute_reply":"2025-05-13T20:34:11.064189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df_2d = pd.DataFrame(X_2d)\nextracted_df_2d[\"labels\"] = Y_2d\nextracted_df_2d.to_csv(features_path_2d, index=False)\nextracted_df_2d.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:34:11.065985Z","iopub.execute_input":"2025-05-13T20:34:11.066247Z","iopub.status.idle":"2025-05-13T20:34:11.078705Z","shell.execute_reply.started":"2025-05-13T20:34:11.066222Z","shell.execute_reply":"2025-05-13T20:34:11.077833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Reading","metadata":{}},{"cell_type":"code","source":"features_path = \"/kaggle/input/zcr-rmse/features_aug.csv\"\nextracted_df = pd.read_csv(features_path)\nprint(extracted_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:06:33.604775Z","iopub.execute_input":"2025-05-13T20:06:33.605227Z","iopub.status.idle":"2025-05-13T20:06:36.916115Z","shell.execute_reply.started":"2025-05-13T20:06:33.605205Z","shell.execute_reply":"2025-05-13T20:06:36.915302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN with 0\nextracted_df = extracted_df.fillna(0)\nprint(extracted_df.isna().any())\nextracted_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:06:36.917072Z","iopub.execute_input":"2025-05-13T20:06:36.917321Z","iopub.status.idle":"2025-05-13T20:06:37.01809Z","shell.execute_reply.started":"2025-05-13T20:06:36.917303Z","shell.execute_reply":"2025-05-13T20:06:37.01734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:42:38.793118Z","iopub.execute_input":"2025-05-13T20:42:38.793835Z","iopub.status.idle":"2025-05-13T20:42:38.811076Z","shell.execute_reply.started":"2025-05-13T20:42:38.793779Z","shell.execute_reply":"2025-05-13T20:42:38.81037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preparation\n\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"X = extracted_df.drop(labels=\"labels\", axis=1)\nY = extracted_df[\"labels\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:01.350173Z","iopub.execute_input":"2025-05-13T20:07:01.350626Z","iopub.status.idle":"2025-05-13T20:07:01.380952Z","shell.execute_reply.started":"2025-05-13T20:07:01.350604Z","shell.execute_reply":"2025-05-13T20:07:01.380367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lb = LabelEncoder()\nY_encoded = lb.fit_transform(Y)\n\nY_tensor = torch.tensor(Y_encoded)\nY_onehot = torch.nn.functional.one_hot(Y_tensor)\n\nY_onehot = Y_onehot.float()\n\nprint(lb.classes_) \nprint(Y_onehot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:03.255012Z","iopub.execute_input":"2025-05-13T20:07:03.255495Z","iopub.status.idle":"2025-05-13T20:07:03.360463Z","shell.execute_reply.started":"2025-05-13T20:07:03.255472Z","shell.execute_reply":"2025-05-13T20:07:03.359928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:03.557303Z","iopub.execute_input":"2025-05-13T20:07:03.557966Z","iopub.status.idle":"2025-05-13T20:07:03.635491Z","shell.execute_reply.started":"2025-05-13T20:07:03.557949Z","shell.execute_reply":"2025-05-13T20:07:03.634718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1, shuffle=True)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:08.926248Z","iopub.execute_input":"2025-05-13T20:07:08.926514Z","iopub.status.idle":"2025-05-13T20:07:08.979753Z","shell.execute_reply.started":"2025-05-13T20:07:08.926495Z","shell.execute_reply":"2025-05-13T20:07:08.979156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:11.01922Z","iopub.execute_input":"2025-05-13T20:07:11.019457Z","iopub.status.idle":"2025-05-13T20:07:11.157753Z","shell.execute_reply.started":"2025-05-13T20:07:11.01944Z","shell.execute_reply":"2025-05-13T20:07:11.157155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We have to use 1-dimensional CNN which need specifical shape:\nX_train = np.expand_dims(X_train, axis=2)\nX_val = np.expand_dims(X_val, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:07:13.640679Z","iopub.execute_input":"2025-05-13T20:07:13.641317Z","iopub.status.idle":"2025-05-13T20:07:13.64611Z","shell.execute_reply.started":"2025-05-13T20:07:13.641295Z","shell.execute_reply":"2025-05-13T20:07:13.645558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2D","metadata":{}},{"cell_type":"code","source":"!pip install torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:44:48.793008Z","iopub.execute_input":"2025-05-13T15:44:48.793291Z","iopub.status.idle":"2025-05-13T15:48:25.968267Z","shell.execute_reply.started":"2025-05-13T15:44:48.793271Z","shell.execute_reply":"2025-05-13T15:48:25.967029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchaudio\nimport torchaudio.transforms as transforms\nimport torch\nfrom tqdm import tqdm\n\ntorch.manual_seed(42)\n\nS_dB_Total = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create transforms once\npower_to_db = transforms.AmplitudeToDB().to(device)\n\n# Assuming all files share the same sample_rate — optional adjustment needed if not\nsample_waveform, sample_rate = torchaudio.load(df[0])\nmel_transform = transforms.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=2048,\n    hop_length=512,\n    n_mels=128\n).to(device)\n\nwith torch.no_grad():\n    for path in tqdm(df.Path):\n        waveform, sr = torchaudio.load(path)\n\n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n\n        waveform = waveform.to(device)\n\n        # If sample_rate differs, reinitialize transform (you can optimize this further)\n        if sr != sample_rate:\n            mel_transform = transforms.MelSpectrogram(\n                sample_rate=sr,\n                n_fft=2048,\n                hop_length=512,\n                n_mels=128\n            ).to(device)\n            sample_rate = sr  # update reference\n\n        S = mel_transform(waveform)\n        S_dB = power_to_db(S)\n\n        S_dB_Total.append(S_dB.squeeze().cpu())  # Store on CPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:51:14.143038Z","iopub.execute_input":"2025-05-13T15:51:14.143339Z","iopub.status.idle":"2025-05-13T15:51:14.180538Z","shell.execute_reply.started":"2025-05-13T15:51:14.143317Z","shell.execute_reply":"2025-05-13T15:51:14.179658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sizes=[]\nfor x in S_dB_Total:\n    sizes.append(x.shape[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:04:17.35795Z","iopub.execute_input":"2025-05-13T13:04:17.358229Z","iopub.status.idle":"2025-05-13T13:04:17.367309Z","shell.execute_reply.started":"2025-05-13T13:04:17.358207Z","shell.execute_reply":"2025-05-13T13:04:17.366668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the bin counts\nbincount_array = np.histogram(sizes, bins=np.arange(min(sizes), max(sizes)+2))[0]\n\n# Create a list of labels for the x-axis\nx_labels = np.arange(min(sizes), max(sizes)+1)\n\n# Plot the bin counts as a bar plot\nplt.bar(x_labels, bincount_array)\n\n# Set labels and title\nplt.xlabel('Numbers')\nplt.ylabel('Count')\nplt.show()\n\nprint(f'min is {min(sizes)}')\nprint(f'max is {max(sizes)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:06:11.986802Z","iopub.execute_input":"2025-05-13T13:06:11.987424Z","iopub.status.idle":"2025-05-13T13:06:12.863817Z","shell.execute_reply.started":"2025-05-13T13:06:11.987402Z","shell.execute_reply":"2025-05-13T13:06:12.863168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\npadding_length = 616\nn_samples = len(S_dB_Total)\nn_mels = S_dB_Total[0].shape[0]  # usually 128\npadded_array = np.zeros((n_samples, n_mels, padding_length), dtype=np.float32)\n\nfor i, array in enumerate(S_dB_Total):\n    array = array.cpu().numpy()\n    cur_len = array.shape[1]\n    padded_array[i, :, :cur_len] = array  # only fill up to cur_len\n\n# Final 3D array\ndata_2D = padded_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:07:03.709271Z","iopub.execute_input":"2025-05-13T13:07:03.709541Z","iopub.status.idle":"2025-05-13T13:07:07.029615Z","shell.execute_reply.started":"2025-05-13T13:07:03.709522Z","shell.execute_reply":"2025-05-13T13:07:07.029074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_2D.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:07:17.618085Z","iopub.execute_input":"2025-05-13T13:07:17.618619Z","iopub.status.idle":"2025-05-13T13:07:17.623015Z","shell.execute_reply.started":"2025-05-13T13:07:17.618598Z","shell.execute_reply":"2025-05-13T13:07:17.622257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Emotion.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:09:30.211062Z","iopub.execute_input":"2025-05-13T13:09:30.211635Z","iopub.status.idle":"2025-05-13T13:09:30.217686Z","shell.execute_reply.started":"2025-05-13T13:09:30.211614Z","shell.execute_reply":"2025-05-13T13:09:30.216891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#normalization\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Reshape the array to 2D\nreshaped_data = data_2D.reshape((-1, 1))\n\n# Initialize the MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\nnormalized_data = scaler.fit_transform(reshaped_data)\n\n# Reshape the normalized data back to the original shape\nnormalized_data = normalized_data.reshape(data_2D.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:33.495417Z","iopub.execute_input":"2025-05-13T13:11:33.496002Z","iopub.status.idle":"2025-05-13T13:11:38.043873Z","shell.execute_reply.started":"2025-05-13T13:11:33.49598Z","shell.execute_reply":"2025-05-13T13:11:38.043257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:45.126703Z","iopub.execute_input":"2025-05-13T13:11:45.127054Z","iopub.status.idle":"2025-05-13T13:11:45.132731Z","shell.execute_reply.started":"2025-05-13T13:11:45.127031Z","shell.execute_reply":"2025-05-13T13:11:45.131845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=np.expand_dims(normalized_data, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:50.745463Z","iopub.execute_input":"2025-05-13T13:11:50.746046Z","iopub.status.idle":"2025-05-13T13:11:50.749561Z","shell.execute_reply.started":"2025-05-13T13:11:50.746004Z","shell.execute_reply":"2025-05-13T13:11:50.748903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels=df.Emotion.replace({'neutral': 1, 'happy': 3, 'sad': 4, 'angry': 5, 'fear': 6, 'disgust': 7, 'surprise': 8}).to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:18:04.194797Z","iopub.execute_input":"2025-05-13T13:18:04.195503Z","iopub.status.idle":"2025-05-13T13:18:04.207068Z","shell.execute_reply.started":"2025-05-13T13:18:04.195481Z","shell.execute_reply":"2025-05-13T13:18:04.206307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:57.98598Z","iopub.execute_input":"2025-05-13T13:11:57.986699Z","iopub.status.idle":"2025-05-13T13:11:57.991256Z","shell.execute_reply.started":"2025-05-13T13:11:57.986672Z","shell.execute_reply":"2025-05-13T13:11:57.990645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:59.265892Z","iopub.execute_input":"2025-05-13T13:11:59.266144Z","iopub.status.idle":"2025-05-13T13:11:59.270746Z","shell.execute_reply.started":"2025-05-13T13:11:59.266126Z","shell.execute_reply":"2025-05-13T13:11:59.270042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save({\n    'data': torch.tensor(data_2D),  \n    'labels': torch.tensor(labels) \n}, 'dataset.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:18:20.047978Z","iopub.execute_input":"2025-05-13T13:18:20.048669Z","iopub.status.idle":"2025-05-13T13:18:28.002085Z","shell.execute_reply.started":"2025-05-13T13:18:20.048646Z","shell.execute_reply":"2025-05-13T13:18:28.001218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Split","metadata":{}},{"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\n# Perform the train-test split (70% train & validation, 30% test )\nX_trainAndVal, X_test, y_trainAndVal, y_test = train_test_split(data, labels, test_size=0.3, stratify=labels, random_state=42)\n\n# Perform the train-validation split (5% test, 95% train)\nX_train, X_val, y_train, y_val = train_test_split(X_trainAndVal, y_trainAndVal, test_size=0.05, stratify=y_trainAndVal, random_state=42)\n\n# Print the sizes of each split\nprint(\"Train set size:\", len(X_train))\nprint(\"Validation set size:\", len(X_val))\nprint(\"Test set size:\", len(X_test))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-13T12:38:12.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass AudioDataset(Dataset):\n    def __init__(self,audio,label):\n        self.audios=audio\n        self.labels=label\n\n    def __len__(self):\n        return len(self.audios)\n\n    def get_batch_imgs(self, idx):\n        # Fetch a batch of inputs\n        return self.audios[idx]\n    \n    def get_batch_labels(self, idx):\n    # Fetch a batch of inputs\n        return self.labels[idx]\n\n    \n    def __getitem__(self, index):\n        audios=self.get_batch_imgs(index)\n        labels=self.get_batch_labels(index)\n        return audios,labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = AudioDataset(X_train,y_train)\nval_dataset = AudioDataset(X_val, y_val)\ntest_dataset = AudioDataset(X_test, y_test)\n\n# Create a DataLoader from the dataset\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader=DataLoader(val_dataset, batch_size=8, shuffle=True)\ntest_dataloader=DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}