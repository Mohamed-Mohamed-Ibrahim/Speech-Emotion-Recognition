{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](attachment:65b9293e-2d96-4789-9bdb-96ad17697d96.png)","metadata":{},"attachments":{"65b9293e-2d96-4789-9bdb-96ad17697d96.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAKAAAACfCAYAAAB+49JVAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABm9SURBVHhe7Z1/bBRnmuc/e+NCS0VD5QYjUSO5uKO5o6Jzz8qdi5sDj8AROCLOaTICewMebA/YBzYGh7FhgKwdZH4zeIgJsY1s5mwQJIsnSkaKJxoTxezGcGNGaUtppDS6NFoa3TS6NJo0Ggotzf34o9vtcru73TYNZez3IxWy6616u4Bvv+9b7/t83+dv/t2/X/T/EAgs4t/EnhAIniZCgAJLEQIUWIoQoMBShAAFliIEKLAUIUCBpQgBCixFCFBgKUKAAksRAhRYihCgwFKEAAWWIgQosBQhQIGlCAEKLEUIUGApQoACSxECFFiKEKDAUoQABZYiBCiwlL9Jry1TYa6uIgOG38PdYGw5oGhkqRDw+HgwuiB6LwCGn9u+eBVYhcJcXcEY89yCxyFtAszIq+VgxYsooYeEAFkC30ArBztdPDJdt7yhk1JbCFd3Fe/1mwooYW/XK2iR+yVpFhLf4env4kT36DqsoYS9XTpD5Q18GlskmDRp6oLz2bzejtF/iJrKCrZVVlDT7UXJK2GdbrpMLiZXC+Lxgj2v0FQwzH2GOsP3bynfwls9PpS8anatt8VeKJgmpKkFLGFvVx53O6s4PTB8TmaOroHPwz0jfCajqJFTeUGOXZCoqZD5XW0T/xQpi9bRVsXpweFzMKfsCCecQU5VH2Zo5PQEkMmuaGSD84dkSmD4v+L9zuNc9UKGXszminzsmc8hcR+/+yNONPdxF0B2sGZ3Oau055H4DnevG7lQi7SAxbzZoXO7D3ILFpEpPSTg7ePY/ovhe5FZWLaHzXkLxnxmsudBzuONhmJWqM/HPI/MwrJaVvMHTk+J3iB9pKkF/JxrPoncsma2rs9jvioDBvc8I+IDlZ84FhH0fs7NwS+4EVrEstejI76E3Ovz4pczyTa3pBMhv5bNTrjWvIVN5dt437Cx7vUCwM66igK0wCfsqy5l85FBDL2YjZGGOWd7Ja8qPs7s2cLmPRe543Ay0g5LyJLGCt3HqfpSatpchLSC6L1z1u9hVx5cO7mNTdWHeD+osamqmoVJnwdytpewAhf7qkvZVPsRAdtPeaMAQGOZ/QUc9iWo0WeYHqRJgH4+a6zj2EAA1VnOwcPttLccYWOBqetUXyFb/Y4b/W5gkEue77DZi5ljriYefgMDGVmJLVDJLipnTdnwUUx2vP8dKfyHrEhAkKv7q9jW3Ae4OVdfwS+P9HLHgEf+L7kTnIVqcwCFrLRJuHuP8ye/wSP/AB987GH0K5HBUG8Xt4PwYLCVK/5ZqLoTUHnVsQD/wGE+dAfB8HD15CDeTJ0VjmTPAxlIIMmEi/p4p7qK9/oAPJyrL2VTfSu3Rz3Ds0+aBAgQ5Eb3Yf6htoJNtf/AaTfkrq9mgz1cOqfQjhaCrKL97G3azxuqBKqdV+OJxowqI2MQ9MUWKMzXbCy2DR8a88eIFOi7yPueELlV73Kmq5Ojh2tZGvlezC2o50BrJ+1dZ2k/Xk2OAkgS8DySFOKBWXGDfgKmXyGI3zXqRAQNRQYtv50zXWfDR+sr2JDCVSd5nj/1XGQoZGdHy1naO06yd3sBc2Orn2akR4B6PqvLCsga/j3oY6jzI4aMeWTZAeystc8j6HNz3evlhtfLDY8LjzGPnKKIQhOwsMiOGvRxzR9b4uGz5gYONQ4fx/nME3sNgJerzbvZVl5KTWMXQ9jZVFVNllpOzXob/p4GaspL2VJZxe+jn/EdoZDEbLOgdYV4+h5LEMMAX/8WNpWXmo7hsW2C5wHw9nF6TxVbyrfxVrcbyf4zdlRNduzxbJAeAQYXkJtfzIYynQwAZOYXvsJi+TvuuAH7yyxWvuVaZwcfdndFjg5Ou74l0/ZyeGwUQ4bq4KXtR9jlkHD1dnAz9oIUmbN+P+2HK5kLPPC5uBydW5SQMLjr8/MIyNDLyY22xv1c84GeV858GcDG8iI7mdFak+HhivdbNEekRQVm59dzoGUPL2Ume55MVjd18naFDQhy1/UFN4ORHhuYrelkaal9BZ4l0vQWDHOLGnmrcBEKhOfxuI+nv4MT3S70ujZ2qC521Hdwz3yTWs7bh53caavi9GB4HtA84RIK3uLSheN8OPgYE9JyHhsP/zeWKQ8xQrOQ+ZbL3Q2cG4Ds7UfY4XieUOghhLx4jRdQ/e/xi+ZBsBWwdfvPcCgPCTGLoOsrDIfCtfIGPo0zJ7i66SyrgpF7sbG8oZ5S23OEQg+RpBDu3uO80+NN8jzGqLlUpFkQ+COnGlu5buhsOL6XbO8hftkWt5l/ZkmbAIeZrelkykH8nnDLMlXIUHVUKc7qiqKRpYYSPu9sTUcOJljVGQ9ZZb4mEYyzepLweSLTV9KolaRCdna8xr3O0VNU04G0C1DwBFArOdCUye8qD/On2LJnnPSMAQVPmK+53PPJtBMfogUUWI1oAQWWIgQosBQhQIGlCAEKLEUIUGApQoACS0m/ABWNLF1jdux5FObqOnPHLGeGz8+JDQ2UVebrOllx7xFMF9I+Dzie58MW+IJd9R2RyOHh86MjoaPryqGHGMxClh7id18ciVYWTBvS2wKm4PkIyk5qypJ4PJy1vFWoEeg/xObKCrZVllLTNkhI/xk7KpLcJ3gmSasAMwp1bIaPD/u8GNqLLI/tVjG4NuBDzStndQItLS+wI3kvcqzbEw0OeDDYwTnXn1Fsr8QN3UqFDL2YrcfbaO86y5muNg7UjQR7Lq5q5sD2EjYcDpe3tx5hQ97Iw8/Or+VAazi49N3D1aze3syBKsfIvXWVbG05y5mu/azOLGFnazMbndHbQa/k7Y4jvGEDbAVs3D0ShDrTSaMAU/F8yCje81wKqPykrCROOL4TXZ2F39s3JjLlZttutu1pnWRcYHL/hyTLZNodyH1N1FQf4qxfYcXr5eEgUbWcXWV2jMFfUVO+jWMuhRWOechyOFJPkmVUux1p4D32HengcuAjrvkVcvLzo5++sMCOFvDwey+gLyFXt5M7veNMUyZ9AkzZ8+Hlw85BAlo+mwvHCjSD+9z1jpzJqQqH8IePanLMlwOoeayM+kLKWVOUF+czk/k/wkh+F6f7/TwyPFzt8xLMVMkGsl63owUGOdXt5gFBbve0cDnGHhDy/oF3ega57fHxAIMrHh+StiTyrHZW2J7H674YjoXsbWJLeQXv9I6uY6aSNgFOyPPh7eDEQBC9sJaXRmnQ4BHPMdfUPQWGQ/gDEpqmjvVIKAtMvhAbi7UFcUPnE/s/wgSD34y6fhhFlsEwTIG0BreD90ddYxij3SKPer7AjY3lBUD+a+TI33Dl46g9UGAiTQKcuOfjbmcXl40XWFdlVuggbv9DNH2ke77dFw7hv2xISAE/N0xXA+A5z3tRX0gDh5rPj3WOJfV/JCdoGCDLplZVJkt5btQ1Y+nnqi+E7iwkx6mB9wuT/1lgJj0CnITnA9yc6/kS7DpqaOTs1Qvh7nlXXWHEjyEzv7CeHU4Fr+ujseJKiWT+j+Tc7vPgz3SyoyKPuZrG4qJaVmixV41lqNdDUHuZNRp4Bk3zUZH5zTHznjOUtAgwu0AnM+AZ06rc63XjU3RWmd8IzQx28Bu3QWQ8H8bbwb6TAwS1v+dg61nOdLVzsEgjONDKsQspNlux+M/zgUtiVcNZ2js6ObVdxYiNhE+Et4OD3S5C9nL2NTSy0e7nknt0FxwX90WGgvNQQx4+Nc+H5leyr66YHNEiwpOYiE4rikaWEozjm5gk4/g/EjFbVQn5R+55aXcn60IdEQNSIlTWHD5Krm+0kSirqpl9qovNjecn9AzTlbS0gE+MoC994iNS3wTFh62Stw7vZ9/uEnJ0Jy9V7KdUB7/HHXtllAxHIWvqalmVeYtLH492sc1XFPy+zyf2DNOY7z3/b3+wL/akwMRfXPyz5/+g2V9ixcoX+Q/SX/jjhYN09v819soof1e4jiU/CHLpwgk++5+mAS7wPflv+WZgkP+d+PYZxdTuggXTnqndBQumPUKAAksRAhRYihCgwFKEAAWWkiYBysyJhM+HDzWyTdvYa2LD6zPUZPc8Lgpzh+0BsoOlZYUsFEtgU4o0TcM42dy6lVzpIUY0xULMpt+Ra+YOlHLoAoCN1U17WKuBEYrcE/yaMycPRzbzTgembdTy6vl1hY6vx6JQqPxq9jqCnIsXLDGDSVMLGMbbP5xiIRxGb9h+xlt1eXFbtjlllazN9HKqNnJP9SF+b9goXR8vlD8NDBznF1bG4akqNpsaN1RsJpNWAZp5MNjFoZ6vkeyv8ZM4kSdapgJBP+7hlTbDw4dHmjjYHc3zMHFkB2uaTkbC7k/yZpG5vy3mzY5GVkZ+m1u0h6ORMPv2lv2scUwgBD/yMwCOag6MV29RI+/mLwD5R9R0NLPBAWBj6fY9ozdyn4E8MQEC0P8l3lAmC03/X8NcH/AQVF/mWFMlLzkj47THXPsdP7WCFN7yVi2nplDD17ONTeVb2OeWWFVUwvxI2Xgh+MM/R06MX29PE9v6b4HxFacq6zjnAtDJtb9ArnNmx+Y/WQES5EFoFlK8fmewhV1H/pHr6JRWHeBUVxtHG8rjvySkFHafSmqFYcKCkWSVDAzudO5my54O7qQYgp+YxPWOpZd3KkvZst+qMcHU4AkLUGG29JCYiPUojzy9/Kaxjm3lW9hxsh9/5svsqiscO2ZMKew+ldQKEfzn+U1/APX1vZzu6uTXxxujXXAqIfgJSVKvID5PVIAZRUvQ8TE0ZliXyeKiclbmDQ8ODe65LvLO4C0k9QXGdEqphN1PKLWCwc3uBn5ZWcrmPa38LpDJq1X1LE0xBF+K26STtF5BfJ6MABWNnPWNHCxchH/gfBw/RADZ5mRdUUk0lQGKkw0OlZDfx+T2gZ9AaoW8Wo627GGpDI/8Lq65A+HpoxRC8H1BA1lbwlIFQCEn3zYi8iT1jiU8LxpOazZzSe88oOnfMmT8maHeLs70DhvMY+YB5Tw2NJSwQh1pXUKBrzjbNpzUbxKknFrBxuqmetZqEkYIZHMaBWB2fjU7XnegymD4B7gUdPIqXeEI6GiaBYD7eNx+VLvEpfHqVQt5s+HvscsP8fRU8KveQnZ2/BQ+ruBXM3gYmCYBPgayynxNIZQowfUkSDm1QoIQ/cmF4JtIUO8o9GqO7la5XN3Ap2N6iJmD9RHRob/y10CAB/8aWzB5HgVTrO9fg9wL/JX/az5nq2Rf0wZW6N/nLwGZrNe3UOr4Pv/rn7u5GhPdnJB49cbiWMlPVD9dvS5SedTpivUt4BQkQy+ktOjHLFZlQgE/Q72tj5etKR72AlYqbj4bmKTTb5ogBCiwlCfzFiwQpIgQoMBShAAFliIEKLAUIUCBpQgBCiwlTdMw4STLY5bojUhCZkUjK86aZ9Dn4Z55FUDRyFIhECfB82TI0PNZ+/rLLAx5+E3z+QRhUQIrSZMAR3tCovj62Lb/Iqzfz5mCBYRCDxlZSwhyrW04ODNM4hQPkyGfra0/Rw98yRX3l/y2ZyDxspjAMtIqwBHDUQzr93MmL8iJ6uNcjy0bRi5mZ4sTfDI2PklDoGYJe7sc3NxTxwcze7FhSjNlxoDjp3iYAEWNvNuRj415rGjq5N2GYgAyHOXsbOkM+zVaj7DZ5MdIlsZhZUMnO6sq2dt6ljOt9WQjs7BsD1vLHGODZwUT4ikKUEriHU4lxcME6GliW2U/Xm7xu8qK8DBALWFXVR6Sq4Wa8i3s6w2yeH01G+yMn8ZBmoXusHHzwiH2nTyPB41l9hdw2JcQx28lmABpFaCtIOwGCx9tbDZvzSu/wKbde9k3fGwvGYl8TjnFA4BKdpHJH1JWTHYKKphT6MAWGOBYt5sHGNzpbeGSbx45+Y6U0jgEXF18MOCJbHDp4Vx9KZvqW+NEZgsmQloF6O0rZVP58DGS+w0A4ytORMtK2WQaD04oxQMK8zWTP8SmMX/M6/dYNEUG9WVOR78g7azVRlI1jJ/GYXJx2oLkpFWAk2OiKR48fNZs8oc0HuezFLQRNAzwf85m85egvDQcZPoYaRwEj4f1ApxUioeJc3vAS0B1sCk/0lwq+Ww+fpKtBZmTSuMwW9PJ0lJoegVJeXoClH/Ejmj3NzJGnHSKh4nibuVYb4DFZe/S3tFJe8vPWRwc4IO+wCTSOOis3b6XmsJxVCoYlzTNAz5LhFdtpHgelFS8HAAUsrPjNe51xoxzBRPm6bWAUwaDe5444mMCaRzUH6LgwyXE99jMQAGmg6+53PMJf4o9LZgwM7ALFkwlRAsosBQhQIGlCAEKLEUIUGApQoACSxECFFhKmgQYPwdIOE9HvPT04fNjr08l38hEMeUKEUw50jQPmCgkv4S9XXncbYtZsipspL1oESHPf2fbEbP5I9ZbMgtZuo+nv4MT3a7xVyjiYt4XUDDVSFMLOBFkljs0gt5vCNl+zPLYYlO+kW2VpdR0+1DzS1gXLzJL8Mzz9AUov0auZnDj4z9wI6SROyqXx1ge9H/ODWMeWY8pQKWokaMdZznT1cnRhuKo3yOZF2RxVTMHtpew4XBbHB9JOO/ImqJGft0Vqbcpsst/Zgk7W5vZaI7k0St5u+MIb9jCO7lu3F3L0pmdIgTSLUBJNo/fdLJ0ecz+yHOKHOiGl8vuQa54Q+j2nyYf58kLmCM9JBQveCBlVJbpPk7Vl1LT5iKkFUT8HuN4QWQZ1ZHHfHcrNeXbODgYIifqI5GQpUWscgZ5f88WNu85z00lsst/4COu+RVy8vOjT7CwwI4W8PB7L6AvIVe3kztmN/aZR1oFqDnr2VVnPpyY9vcGVFbpPyTg/ZybwPU+DwHNPiaTkqIN+z0q2dpUgG54+DSOTzirwOwNKeelhK2kwVBvF7eD8GCwlSv+Wai6MyUvCAEXpy9EcoZ0t3I5MI/svGHlPMTT1xLJS9LP6d6vCWkvsgyDKx4fkraEHADsrLA9j9d9MZz+obeJLVamDZtCpFWAI2O34aOfUfuNq6+Roz5EUovZ27SfvUUqEj8kJyawU84c8XtI/n6ONR7nepx9lDPNuUNsNvS4W+IDBPGbDPBmxvWC+L825Qzxc91/H0kefn0PcNucgmLAT0CSkIFHPV/gxsbyAiD/NXLkb7jycZy/xAwnrQIcj4VFdlTDxzVPxPvh9XDNex/VXjwq9N7vGvF8vNN8nhsJut+hNrM3pIFzcVrJpKTgBVEyF5mGCDKLM5+D0PD+DgqZ5jRkjkwUiOz+0M9VXwjdWUiOUwPvF3HSVQieogDD3ZDf1cEHUe9HFx90uvArNlYk7D6fJCl4QVQHm/LCL0oZedUsU+/jdQ83p8+RU1ASeWmxsbpAR/F7uRYpHer1ENReZo0GnkHTt0NWmR93fnTm8fQE6HyFbOVbrvfGNDH+TxjyP092QbrMHxMgBS9I0ONh9vp22js6OV2hYwye53RUS7cY8js42NVJe9fbrM30cbaza6TLdl9kKDgPNRQzhs2vZF9dMTmiRUzXRPQzTgIvSHZdGxvp4hfNbuboGozazcs0wa1oZCnBOJk+VdYcPkqu7xC/bBvxjmZVNbNPdbG58fwkJ9enD0+vBZzKjOsFCftIRm0lZyZOmtkMRyFr6mpZlXmLSx+PNi7PVxT8vs+TfN7MQQgwCUG/F68/tlUb5s/c9vkT7jlod7zIYjnAb9sO81nMqOOOq48PYociMxTRBQssRbSAAksRAhRYihCgwFKEAAWWIgQosBQhQIGlpGkaJrzjlBz0ccdvnq2N2Ykqab6QRLtWKczVVRhz/tkn5czu05g0ZUx3UN5UR+lyG/7+Af4cTQYSPv/jv/2IL9zAmnpOlP1Xljmd/JelS1m2dCnLlv4nvv8vfXzlj7k2ylp2HP85//l7seenAPnV7F3zH/mX/+E2hWylzqq6X/PaD6bg3+spksYu+CFBbKyrykse4Wx8xalRMYOjk9U8U6gqNps6NkOUIGXSKEDwDboI2Yuj4UtThZUNnbxZVsybwzlCWhpZbvJjJM4fEvZ9rBy5lJUNnRyocoRzkeQvAPlH1HQ0s8ER8ZDUVbK15SxnuvazehzPiSCtApyFHOrinFsit6iS7NjiKMnyhTwZJGkWulPndmcdm2rf40pIY936iPEjaf4QCVmSRvlaJGkWsiyFc5H034q06OFWXJJlVLsdaeA99h3p4PI4nhNBWgUIYHC97SPc0otsqEoQYZosX8hEUfNYafKErCnKS5BbBAzPJ3zoDkJwkHMuP5L6Atnj5g+ZOCHvH3inZ5DbHh8PUvGczHDSLEDA6OPUx18jO8vDFsRYkuQLmTDKglGekMXagoTjsUAg/kBzvPwhE8UwAqN+H89zMtNJvwCBR30t/NarsKLsxxPYEiNEKASyHBMTr8rI3OfuKHdTBM953jN5Qg41n59w5qKk+UMAkFFiw/RTJQXPyUzniQgQDP6psx+fqmNL+cvu4pr3O1RnLW84h3N52FlT5UQNerjyhDYET54/5FuM0DyyC3UyEnlGkpKC5yTC3IJatlYVJBxCTFeekADDfovT/f6xvU2CfCEA10+2cMYjsaLq3XBZy05WKT7OnmyZfDc9Hsnyh9DHuf5bKHl7Od11llNVCnfMPWz/H3HzI3Z0dbIz3otFCp6TYRY77TgcS5K8vE1P0rQSkm6sWP1ItBKTBhJ4TgRTVoCCmcKT64IFghQQAhRYihCgwFKEAAWWIgQosBQhQIGlCAEKLEUIUGApQoACSxECFFiKEKDAUoQABZYiBCiwFCFAgaUIAQosRQhQYClCgAJLEQIUWIoQoMBShAAFliIEKLAUIUCBpQgBCixFCFBgKUKAAkv5/3rJvqi2gNNRAAAAAElFTkSuQmCC"}}},{"cell_type":"markdown","source":"# Resourses\n- https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d\n- https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition\n- https://librosa.org/doc/latest/index.html\n- https://www.kaggle.com/code/hossamemamo/speech-emotion-recognition-2-parallel-cnn-conv-2d\n- ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Audio\n# from entropy import spectral_entropy\n# from keras import layers\n# from keras import models\n# from keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n# from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport itertools, torch, sys\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:02.073942Z","iopub.execute_input":"2025-05-13T15:43:02.074564Z","iopub.status.idle":"2025-05-13T15:43:06.975212Z","shell.execute_reply.started":"2025-05-13T15:43:02.074539Z","shell.execute_reply":"2025-05-13T15:43:06.974657Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Paths to\nRavdess = \"../input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24\"\nCrema = \"../input/speech-emotion-recognition-en/Crema\"\nSavee = \"../input/speech-emotion-recognition-en/Savee\"\nTess = \"../input/speech-emotion-recognition-en/Tess\"\nprint(os.listdir(Ravdess))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:06.976444Z","iopub.execute_input":"2025-05-13T15:43:06.976811Z","iopub.status.idle":"2025-05-13T15:43:06.991648Z","shell.execute_reply.started":"2025-05-13T15:43:06.976793Z","shell.execute_reply":"2025-05-13T15:43:06.991107Z"}},"outputs":[{"name":"stdout","text":"['Actor_02', 'Actor_17', 'Actor_05', 'Actor_16', 'Actor_21', 'Actor_01', 'Actor_11', 'Actor_20', 'Actor_08', 'Actor_15', 'Actor_06', 'Actor_12', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_04', 'Actor_19', 'Actor_10', 'Actor_09', 'Actor_14', 'Actor_03', 'Actor_13', 'Actor_18', 'Actor_07']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nemotion_df = []\n\nfor dir in ravdess_directory_list:\n    actor = os.listdir(os.path.join(Ravdess, dir))\n    for wav in actor:\n        info = wav.partition(\".wav\")[0].split(\"-\")\n        emotion = int(info[2])\n        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:06.992359Z","iopub.execute_input":"2025-05-13T15:43:06.992639Z","iopub.status.idle":"2025-05-13T15:43:07.367361Z","shell.execute_reply.started":"2025-05-13T15:43:06.992595Z","shell.execute_reply":"2025-05-13T15:43:07.366563Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"Ravdess_df = pd.DataFrame.from_dict(emotion_df)\nRavdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:07.368763Z","iopub.execute_input":"2025-05-13T15:43:07.369124Z","iopub.status.idle":"2025-05-13T15:43:07.381350Z","shell.execute_reply.started":"2025-05-13T15:43:07.369098Z","shell.execute_reply":"2025-05-13T15:43:07.380544Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:07.483933Z","iopub.execute_input":"2025-05-13T15:43:07.484462Z","iopub.status.idle":"2025-05-13T15:43:07.515812Z","shell.execute_reply.started":"2025-05-13T15:43:07.484434Z","shell.execute_reply":"2025-05-13T15:43:07.515078Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"    Emotion                                               Path\n0  surprise  ../input/speech-emotion-recognition-en/Ravdess...\n1   neutral  ../input/speech-emotion-recognition-en/Ravdess...\n2   disgust  ../input/speech-emotion-recognition-en/Ravdess...\n3   disgust  ../input/speech-emotion-recognition-en/Ravdess...\n4   neutral  ../input/speech-emotion-recognition-en/Ravdess...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"emotion_df = []\n\nfor wav in os.listdir(Crema):\n    info = wav.partition(\".wav\")[0].split(\"_\")\n    if info[2] == 'SAD':\n        emotion_df.append((\"sad\", Crema + \"/\" + wav))\n    elif info[2] == 'ANG':\n        emotion_df.append((\"angry\", Crema + \"/\" + wav))\n    elif info[2] == 'DIS':\n        emotion_df.append((\"disgust\", Crema + \"/\" + wav))\n    elif info[2] == 'FEA':\n        emotion_df.append((\"fear\", Crema + \"/\" + wav))\n    elif info[2] == 'HAP':\n        emotion_df.append((\"happy\", Crema + \"/\" + wav))\n    elif info[2] == 'NEU':\n        emotion_df.append((\"neutral\", Crema + \"/\" + wav))\n    else:\n        emotion_df.append((\"unknown\", Crema + \"/\" + wav))\n\n\nCrema_df = pd.DataFrame.from_dict(emotion_df)\nCrema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nCrema_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:12.098335Z","iopub.execute_input":"2025-05-13T15:43:12.098684Z","iopub.status.idle":"2025-05-13T15:43:12.331902Z","shell.execute_reply.started":"2025-05-13T15:43:12.098660Z","shell.execute_reply":"2025-05-13T15:43:12.331145Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Emotion                                               Path\n0  disgust  ../input/speech-emotion-recognition-en/Crema/1...\n1    happy  ../input/speech-emotion-recognition-en/Crema/1...\n2    happy  ../input/speech-emotion-recognition-en/Crema/1...\n3  disgust  ../input/speech-emotion-recognition-en/Crema/1...\n4  disgust  ../input/speech-emotion-recognition-en/Crema/1...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Crema/1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happy</td>\n      <td>../input/speech-emotion-recognition-en/Crema/1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>../input/speech-emotion-recognition-en/Crema/1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Crema/1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Crema/1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nemotion_df = []\n\nfor dir in tess_directory_list:\n    for wav in os.listdir(os.path.join(Tess, dir)):\n        info = wav.partition(\".wav\")[0].split(\"_\")\n        emo = info[2]\n        if emo == \"ps\":\n            emotion_df.append((\"surprise\", os.path.join(Tess, dir, wav)))\n        else:\n            emotion_df.append((emo, os.path.join(Tess, dir, wav)))\n\n\nTess_df = pd.DataFrame.from_dict(emotion_df)\nTess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nTess_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:12.333132Z","iopub.execute_input":"2025-05-13T15:43:12.333409Z","iopub.status.idle":"2025-05-13T15:43:12.579539Z","shell.execute_reply.started":"2025-05-13T15:43:12.333387Z","shell.execute_reply":"2025-05-13T15:43:12.578928Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  Emotion                                               Path\n0    fear  ../input/speech-emotion-recognition-en/Tess/YA...\n1    fear  ../input/speech-emotion-recognition-en/Tess/YA...\n2    fear  ../input/speech-emotion-recognition-en/Tess/YA...\n3    fear  ../input/speech-emotion-recognition-en/Tess/YA...\n4    fear  ../input/speech-emotion-recognition-en/Tess/YA...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Tess/YA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Tess/YA...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Tess/YA...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Tess/YA...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Tess/YA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"savee_directiory_list = os.listdir(Savee)\n\nemotion_df = []\n\nfor wav in savee_directiory_list:\n    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n    emotion = re.split(r\"[0-9]\", info)[0]\n    if emotion=='a':\n        emotion_df.append((\"angry\", Savee + \"/\" + wav))\n    elif emotion=='d':\n        emotion_df.append((\"disgust\", Savee + \"/\" + wav))\n    elif emotion=='f':\n        emotion_df.append((\"fear\", Savee + \"/\" + wav))\n    elif emotion=='h':\n        emotion_df.append((\"happy\", Savee + \"/\" + wav))\n    elif emotion=='n':\n        emotion_df.append((\"neutral\", Savee + \"/\" + wav))\n    elif emotion=='sa':\n        emotion_df.append((\"sad\", Savee + \"/\" + wav))\n    else:\n        emotion_df.append((\"surprise\", Savee + \"/\" + wav))\n\n\nSavee_df = pd.DataFrame.from_dict(emotion_df)\nSavee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nSavee_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:13.929990Z","iopub.execute_input":"2025-05-13T15:43:13.930276Z","iopub.status.idle":"2025-05-13T15:43:13.954198Z","shell.execute_reply.started":"2025-05-13T15:43:13.930256Z","shell.execute_reply":"2025-05-13T15:43:13.953510Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   Emotion                                               Path\n0    happy  ../input/speech-emotion-recognition-en/Savee/J...\n1     fear  ../input/speech-emotion-recognition-en/Savee/K...\n2    happy  ../input/speech-emotion-recognition-en/Savee/D...\n3  disgust  ../input/speech-emotion-recognition-en/Savee/D...\n4    angry  ../input/speech-emotion-recognition-en/Savee/K...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>happy</td>\n      <td>../input/speech-emotion-recognition-en/Savee/J...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>../input/speech-emotion-recognition-en/Savee/K...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>../input/speech-emotion-recognition-en/Savee/D...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Savee/D...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angry</td>\n      <td>../input/speech-emotion-recognition-en/Savee/K...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:14.303249Z","iopub.execute_input":"2025-05-13T15:43:14.303952Z","iopub.status.idle":"2025-05-13T15:43:14.309854Z","shell.execute_reply.started":"2025-05-13T15:43:14.303928Z","shell.execute_reply":"2025-05-13T15:43:14.309287Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(12162, 2)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"%matplotlib inline\n\nplt.style.use(\"ggplot\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:15.798286Z","iopub.execute_input":"2025-05-13T15:43:15.799022Z","iopub.status.idle":"2025-05-13T15:43:15.803179Z","shell.execute_reply.started":"2025-05-13T15:43:15.798998Z","shell.execute_reply":"2025-05-13T15:43:15.802419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"plt.title(\"Count of emotions:\")\nsns.countplot(x=df[\"Emotion\"])\nsns.despine(top=True, right=True, left=False, bottom=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:16.227814Z","iopub.execute_input":"2025-05-13T15:43:16.228510Z","iopub.status.idle":"2025-05-13T15:43:16.483194Z","shell.execute_reply.started":"2025-05-13T15:43:16.228488Z","shell.execute_reply":"2025-05-13T15:43:16.482440Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:26.981570Z","iopub.execute_input":"2025-05-13T15:43:26.982276Z","iopub.status.idle":"2025-05-13T15:43:26.987589Z","shell.execute_reply.started":"2025-05-13T15:43:26.982252Z","shell.execute_reply":"2025-05-13T15:43:26.986801Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"emotion='fear'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:40:33.202592Z","iopub.execute_input":"2025-05-13T12:40:33.202860Z","iopub.status.idle":"2025-05-13T12:40:45.606956Z","shell.execute_reply.started":"2025-05-13T12:40:33.202841Z","shell.execute_reply":"2025-05-13T12:40:45.606214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='angry'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:40:45.607987Z","iopub.execute_input":"2025-05-13T12:40:45.608372Z","iopub.status.idle":"2025-05-13T12:40:46.636681Z","shell.execute_reply.started":"2025-05-13T12:40:45.608354Z","shell.execute_reply":"2025-05-13T12:40:46.635929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='sad'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:19:43.040382Z","iopub.execute_input":"2025-05-13T05:19:43.040752Z","iopub.status.idle":"2025-05-13T05:19:44.164190Z","shell.execute_reply.started":"2025-05-13T05:19:43.040721Z","shell.execute_reply":"2025-05-13T05:19:44.163140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data augmentation\n\nWe have some ways for data augmentation in sound data:\n\n1. Noise injection\n2. Stretching\n3. Shifting\n4. Pitching","metadata":{}},{"cell_type":"code","source":"def noise(data, random=False, rate=0.035, threshold=0.075):\n    \"\"\"Add some noise to sound sample. Use random if you want to add random noise with some threshold.\n    Or use rate Random=False and rate for always adding fixed noise.\"\"\"\n    if random:\n        rate = np.random.random() * threshold\n    noise_amp = rate*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    \"\"\"Stretching data with some rate.\"\"\"\n    return librosa.effects.time_stretch(data, rate=rate)\n\ndef shift(data, rate=1000):\n    \"\"\"Shifting data with some rate\"\"\"\n    shift_range = int(np.random.uniform(low=-5, high = 5)*rate)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7, random=False):\n    \"\"\"\"Add some pitch to sound sample. Use random if you want to add random pitch with some threshold.\n    Or use pitch_factor Random=False and rate for always adding fixed pitch.\"\"\"\n    if random:\n        pitch_factor=np.random.random() * pitch_factor\n    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:20:46.513447Z","iopub.execute_input":"2025-05-13T11:20:46.513836Z","iopub.status.idle":"2025-05-13T11:20:46.520877Z","shell.execute_reply.started":"2025-05-13T11:20:46.513817Z","shell.execute_reply":"2025-05-13T11:20:46.520206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:41.597408Z","iopub.execute_input":"2025-05-13T15:43:41.597730Z","iopub.status.idle":"2025-05-13T15:43:41.605212Z","shell.execute_reply.started":"2025-05-13T15:43:41.597708Z","shell.execute_reply":"2025-05-13T15:43:41.604560Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"    Emotion                                               Path\n0  surprise  ../input/speech-emotion-recognition-en/Ravdess...\n1   neutral  ../input/speech-emotion-recognition-en/Ravdess...\n2   disgust  ../input/speech-emotion-recognition-en/Ravdess...\n3   disgust  ../input/speech-emotion-recognition-en/Ravdess...\n4   neutral  ../input/speech-emotion-recognition-en/Ravdess...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>../input/speech-emotion-recognition-en/Ravdess...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"path = df[df[\"Emotion\"] == \"happy\"][\"Path\"].iloc[0]\ndata, sampling_rate = librosa.load(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:43:42.676601Z","iopub.execute_input":"2025-05-13T15:43:42.677301Z","iopub.status.idle":"2025-05-13T15:43:54.187374Z","shell.execute_reply.started":"2025-05-13T15:43:42.677280Z","shell.execute_reply":"2025-05-13T15:43:54.186794Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveshow(data, sr=sampling_rate)\nAudio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:20:50.390650Z","iopub.execute_input":"2025-05-13T11:20:50.390939Z","iopub.status.idle":"2025-05-13T11:20:50.825997Z","shell.execute_reply.started":"2025-05-13T11:20:50.390919Z","shell.execute_reply":"2025-05-13T11:20:50.825270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"noised_data = noise(data, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=noised_data, sr=sampling_rate)\nAudio(noised_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:02:04.371810Z","iopub.execute_input":"2025-05-12T18:02:04.372115Z","iopub.status.idle":"2025-05-12T18:02:04.722574Z","shell.execute_reply.started":"2025-05-12T18:02:04.372095Z","shell.execute_reply":"2025-05-12T18:02:04.721932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stretched_data = stretch(data, rate=0.5)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=stretched_data, sr=sampling_rate)\nAudio(stretched_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:02:52.277361Z","iopub.execute_input":"2025-05-12T18:02:52.277646Z","iopub.status.idle":"2025-05-12T18:02:52.974059Z","shell.execute_reply.started":"2025-05-12T18:02:52.277630Z","shell.execute_reply":"2025-05-12T18:02:52.973325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shifted_data = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=shifted_data, sr=sampling_rate)\nAudio(shifted_data, rate=sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:03:11.085446Z","iopub.execute_input":"2025-05-12T18:03:11.086180Z","iopub.status.idle":"2025-05-12T18:03:11.396191Z","shell.execute_reply.started":"2025-05-12T18:03:11.086160Z","shell.execute_reply":"2025-05-12T18:03:11.395498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pitched_data = pitch(data, sampling_rate, pitch_factor=0.5, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=pitched_data, sr=sampling_rate)\nAudio(pitched_data, rate=sampling_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:03:18.032236Z","iopub.execute_input":"2025-05-12T18:03:18.032526Z","iopub.status.idle":"2025-05-12T18:03:18.353313Z","shell.execute_reply.started":"2025-05-12T18:03:18.032499Z","shell.execute_reply":"2025-05-12T18:03:18.352671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For our data augmentation we will use noise and pitch and combination with both of it.","metadata":{}},{"cell_type":"markdown","source":"## Feature extraction\n\n#### There are some features may be useful:","metadata":{}},{"cell_type":"markdown","source":"1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n3. Entropy of Energy :The entropy of sub-frames normalized energies. It can be interpreted as a measure of abrupt changes.\n3. Spectral Centroid : The center of gravity of the spectrum.\n4. Spectral Spread : The second central moment of the spectrum.\n5. Spectral Entropy : Entropy of the normalized spectral energies for a set of sub-frames.\n6. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n7. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n8. MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.","metadata":{}},{"cell_type":"code","source":"n_fft = 2048\nhop_length = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:44:02.565095Z","iopub.execute_input":"2025-05-13T15:44:02.565530Z","iopub.status.idle":"2025-05-13T15:44:02.569252Z","shell.execute_reply.started":"2025-05-13T15:44:02.565508Z","shell.execute_reply":"2025-05-13T15:44:02.568508Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def chunks(data, frame_length, hop_length):\n    for i in range(0, len(data), hop_length):\n        yield data[i:i+frame_length]\n\n# Zero Crossing Rate\ndef zcr(data, frame_length=2048, hop_length=512):\n    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(zcr)\n\n\ndef energy(data, frame_length=2048, hop_length=512):\n    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])\n    return en / frame_length\n\n\ndef rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n\n\ndef entropy_of_energy(data, frame_length=2048, hop_length=512):\n    energies = energy(data, frame_length, hop_length)\n    energies /= np.sum(energies)\n\n    entropy = 0.0\n    entropy -= energies * np.log2(energies)\n    return entropy\n\n\ndef spc(data, sr, frame_length=2048, hop_length=512):\n    spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spectral_centroid)\n\n\n# def spc_entropy(data, sr):\n#     spc_en = spectral_entropy(data, sf=sr, method=\"fft\")\n#     return spc_en\n\ndef spc_flux(data):\n    isSpectrum = data.ndim == 1\n    if isSpectrum:\n        data = np.expand_dims(data, axis=1)\n\n    X = np.c_[data[:, 0], data]\n    af_Delta_X = np.diff(X, 1, axis=1)\n    vsf = np.sqrt((np.power(af_Delta_X, 2).sum(axis=0))) / X.shape[0]\n\n    return np.squeeze(vsf) if isSpectrum else vsf\n\n\ndef spc_rollof(data, sr, frame_length=2048, hop_length=512):\n    spcrollof = librosa.feature.spectral_rolloff(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spcrollof)\n\n\ndef chroma_stft(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sr)\n    return np.squeeze(chroma_stft.T) if not flatten else np.ravel(chroma_stft.T)\n\n\ndef mel_spc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mel = librosa.feature.melspectrogram(y=data, sr=sr)\n    return np.squeeze(mel.T) if not flatten else np.ravel(mel.T)\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:21:00.148396Z","iopub.execute_input":"2025-05-13T11:21:00.148722Z","iopub.status.idle":"2025-05-13T11:21:00.163225Z","shell.execute_reply.started":"2025-05-13T11:21:00.148701Z","shell.execute_reply":"2025-05-13T11:21:00.162368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = np.array(df[\"Path\"])[658]\ndata, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\nlen(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:21:09.542562Z","iopub.execute_input":"2025-05-13T11:21:09.543355Z","iopub.status.idle":"2025-05-13T11:21:09.570995Z","shell.execute_reply.started":"2025-05-13T11:21:09.543330Z","shell.execute_reply":"2025-05-13T11:21:09.570246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"ZCR: \", zcr(data).shape)\nprint(\"Energy: \", energy(data).shape)\nprint(\"Entropy of Energy :\", entropy_of_energy(data).shape)\nprint(\"RMS :\", rmse(data).shape)\nprint(\"Spectral Centroid :\", spc(data, sampling_rate).shape)\n# print(\"Spectral Entropy: \", spc_entropy(data, sampling_rate).shape)\nprint(\"Spectral Flux: \", spc_flux(data).shape)\nprint(\"Spectral Rollof: \", spc_rollof(data, sampling_rate).shape)\nprint(\"Chroma STFT: \", chroma_stft(data, sampling_rate).shape)\nprint(\"MelSpectrogram: \", mel_spc(data, sampling_rate).shape)\nprint(\"MFCC: \", mfcc(data, sampling_rate).shape)\nprint(\"Melspectrogram: \",np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0).shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:21:10.762573Z","iopub.execute_input":"2025-05-13T11:21:10.762849Z","iopub.status.idle":"2025-05-13T11:21:12.182084Z","shell.execute_reply.started":"2025-05-13T11:21:10.762832Z","shell.execute_reply":"2025-05-13T11:21:12.181115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(data, sr, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        # np.mean(energy(data, frame_length, hop_length),axis=0),\n                        # np.mean(entropy_of_energy(data, frame_length, hop_length), axis=0),\n                        rmse(data, frame_length, hop_length),\n                        # spc(data, sr, frame_length, hop_length),\n                        # spc_entropy(data, sr),\n                        # spc_flux(data),\n                        # spc_rollof(data, sr, frame_length, hop_length),\n                        # chroma_stft(data, sr, frame_length, hop_length),\n                        # mel_spc(data, sr, frame_length, hop_length, flatten=True)\n                        # mfcc(data, sr, frame_length, hop_length)\n                                    ))\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:01:59.366884Z","iopub.execute_input":"2025-05-13T11:01:59.367829Z","iopub.status.idle":"2025-05-13T11:01:59.372826Z","shell.execute_reply.started":"2025-05-13T11:01:59.367783Z","shell.execute_reply":"2025-05-13T11:01:59.372022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_features(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n     # without augmentation\n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n    # data with noise\n    noise_data = noise(data, random=True)\n    res2 = extract_features(noise_data, sample_rate)\n    result = np.vstack((result, res2)) # stacking vertically\n\n    # data with pitching\n    pitched_data = pitch(data, sample_rate, random=True)\n    res3 = extract_features(pitched_data, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    # data with pitching and white_noise\n    new_data = pitch(data, sample_rate, random=True)\n    data_noise_pitch = noise(new_data, random=True)\n    res3 = extract_features(data_noise_pitch, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:02:01.052479Z","iopub.execute_input":"2025-05-13T11:02:01.052833Z","iopub.status.idle":"2025-05-13T11:02:01.059351Z","shell.execute_reply.started":"2025-05-13T11:02:01.052779Z","shell.execute_reply":"2025-05-13T11:02:01.058467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Y = [], []\nprint(\"Feature processing...\")\nfor path, emotion, ind in zip(df.Path, df.Emotion, range(df.Path.shape[0])):\n    features = get_features(path)\n    # print(features.shape)\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n     \n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n    # print(len(X[0]))\n    # print(len(Y))\n    # break\nprint(\"Done.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's save our features as DataFrame for further processing:","metadata":{}},{"cell_type":"code","source":"features_path = \"./features.csv\"\nfeatures_path_2d = \"./features_2d.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T06:49:25.238977Z","iopub.status.idle":"2025-05-13T06:49:25.239281Z","shell.execute_reply.started":"2025-05-13T06:49:25.239142Z","shell.execute_reply":"2025-05-13T06:49:25.239157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df = pd.DataFrame(X)\nextracted_df[\"labels\"] = Y\nextracted_df.to_csv(features_path, index=False)\nextracted_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T06:49:25.240286Z","iopub.status.idle":"2025-05-13T06:49:25.240553Z","shell.execute_reply.started":"2025-05-13T06:49:25.240431Z","shell.execute_reply":"2025-05-13T06:49:25.240443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df_2d = pd.DataFrame(X_2d)\nextracted_df_2d[\"labels\"] = Y_2d\nextracted_df_2d.to_csv(features_path_2d, index=False)\nextracted_df_2d.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T06:49:25.242212Z","iopub.status.idle":"2025-05-13T06:49:25.242504Z","shell.execute_reply.started":"2025-05-13T06:49:25.242366Z","shell.execute_reply":"2025-05-13T06:49:25.242379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Reading","metadata":{}},{"cell_type":"code","source":"extracted_df = pd.read_csv(features_path)\nprint(extracted_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:15:46.223245Z","iopub.execute_input":"2025-05-11T15:15:46.223524Z","iopub.status.idle":"2025-05-11T15:16:14.280845Z","shell.execute_reply.started":"2025-05-11T15:15:46.223500Z","shell.execute_reply":"2025-05-11T15:16:14.279781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN with 0\nextracted_df = extracted_df.fillna(0)\nprint(extracted_df.isna().any())\nextracted_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:11:41.218597Z","iopub.execute_input":"2025-05-11T15:11:41.218856Z","iopub.status.idle":"2025-05-11T15:11:42.294303Z","shell.execute_reply.started":"2025-05-11T15:11:41.218837Z","shell.execute_reply":"2025-05-11T15:11:42.293334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extracted_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:11:42.295262Z","iopub.execute_input":"2025-05-11T15:11:42.295608Z","iopub.status.idle":"2025-05-11T15:11:42.317878Z","shell.execute_reply.started":"2025-05-11T15:11:42.295585Z","shell.execute_reply":"2025-05-11T15:11:42.316787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preparation\n\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"X = extracted_df.drop(labels=\"labels\", axis=1)\nY = extracted_df[\"labels\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:19:22.432798Z","iopub.execute_input":"2025-05-11T15:19:22.435703Z","iopub.status.idle":"2025-05-11T15:19:22.989030Z","shell.execute_reply.started":"2025-05-11T15:19:22.435631Z","shell.execute_reply":"2025-05-11T15:19:22.987691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lb = LabelEncoder()\nY_encoded = lb.fit_transform(Y)\n\nY_tensor = torch.tensor(Y_encoded)\nY_onehot = torch.nn.functional.one_hot(Y_tensor)\n\nY_onehot = Y_onehot.float()\n\nprint(lb.classes_) \nprint(Y_onehot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:21:54.300155Z","iopub.execute_input":"2025-05-11T15:21:54.300544Z","iopub.status.idle":"2025-05-11T15:21:54.471582Z","shell.execute_reply.started":"2025-05-11T15:21:54.300517Z","shell.execute_reply":"2025-05-11T15:21:54.470573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:21:57.318951Z","iopub.execute_input":"2025-05-11T15:21:57.319301Z","iopub.status.idle":"2025-05-11T15:21:58.290426Z","shell.execute_reply.started":"2025-05-11T15:21:57.319274Z","shell.execute_reply":"2025-05-11T15:21:58.289486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1, shuffle=True)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:21:59.188575Z","iopub.execute_input":"2025-05-11T15:21:59.188954Z","iopub.status.idle":"2025-05-11T15:21:59.910898Z","shell.execute_reply.started":"2025-05-11T15:21:59.188927Z","shell.execute_reply":"2025-05-11T15:21:59.909638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:22:01.371868Z","iopub.execute_input":"2025-05-11T15:22:01.372215Z","iopub.status.idle":"2025-05-11T15:22:07.274882Z","shell.execute_reply.started":"2025-05-11T15:22:01.372190Z","shell.execute_reply":"2025-05-11T15:22:07.274076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We have to use 1-dimensional CNN which need specifical shape:\nX_train = np.expand_dims(X_train, axis=2)\nX_val = np.expand_dims(X_val, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:22:07.275946Z","iopub.execute_input":"2025-05-11T15:22:07.276240Z","iopub.status.idle":"2025-05-11T15:22:07.284742Z","shell.execute_reply.started":"2025-05-11T15:22:07.276218Z","shell.execute_reply":"2025-05-11T15:22:07.283582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2D","metadata":{}},{"cell_type":"code","source":"!pip install torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:44:48.793008Z","iopub.execute_input":"2025-05-13T15:44:48.793291Z","iopub.status.idle":"2025-05-13T15:48:25.968267Z","shell.execute_reply.started":"2025-05-13T15:44:48.793271Z","shell.execute_reply":"2025-05-13T15:48:25.967029Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torchaudio\nimport torchaudio.transforms as transforms\nimport torch\nfrom tqdm import tqdm\n\ntorch.manual_seed(42)\n\nS_dB_Total = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create transforms once\npower_to_db = transforms.AmplitudeToDB().to(device)\n\n# Assuming all files share the same sample_rate  optional adjustment needed if not\nsample_waveform, sample_rate = torchaudio.load(df[0])\nmel_transform = transforms.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=2048,\n    hop_length=512,\n    n_mels=128\n).to(device)\n\nwith torch.no_grad():\n    for path in tqdm(df.Path):\n        waveform, sr = torchaudio.load(path)\n\n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n\n        waveform = waveform.to(device)\n\n        # If sample_rate differs, reinitialize transform (you can optimize this further)\n        if sr != sample_rate:\n            mel_transform = transforms.MelSpectrogram(\n                sample_rate=sr,\n                n_fft=2048,\n                hop_length=512,\n                n_mels=128\n            ).to(device)\n            sample_rate = sr  # update reference\n\n        S = mel_transform(waveform)\n        S_dB = power_to_db(S)\n\n        S_dB_Total.append(S_dB.squeeze().cpu())  # Store on CPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:51:14.143038Z","iopub.execute_input":"2025-05-13T15:51:14.143339Z","iopub.status.idle":"2025-05-13T15:51:14.180538Z","shell.execute_reply.started":"2025-05-13T15:51:14.143317Z","shell.execute_reply":"2025-05-13T15:51:14.179658Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1744263269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Assuming all files share the same sample_rate  optional adjustment needed if not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msample_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m mel_transform = transforms.MelSpectrogram(\n\u001b[1;32m     19\u001b[0m     \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0"],"ename":"KeyError","evalue":"0","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"sizes=[]\nfor x in S_dB_Total:\n    sizes.append(x.shape[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:04:17.357950Z","iopub.execute_input":"2025-05-13T13:04:17.358229Z","iopub.status.idle":"2025-05-13T13:04:17.367309Z","shell.execute_reply.started":"2025-05-13T13:04:17.358207Z","shell.execute_reply":"2025-05-13T13:04:17.366668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the bin counts\nbincount_array = np.histogram(sizes, bins=np.arange(min(sizes), max(sizes)+2))[0]\n\n# Create a list of labels for the x-axis\nx_labels = np.arange(min(sizes), max(sizes)+1)\n\n# Plot the bin counts as a bar plot\nplt.bar(x_labels, bincount_array)\n\n# Set labels and title\nplt.xlabel('Numbers')\nplt.ylabel('Count')\nplt.show()\n\nprint(f'min is {min(sizes)}')\nprint(f'max is {max(sizes)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:06:11.986802Z","iopub.execute_input":"2025-05-13T13:06:11.987424Z","iopub.status.idle":"2025-05-13T13:06:12.863817Z","shell.execute_reply.started":"2025-05-13T13:06:11.987402Z","shell.execute_reply":"2025-05-13T13:06:12.863168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\npadding_length = 616\nn_samples = len(S_dB_Total)\nn_mels = S_dB_Total[0].shape[0]  # usually 128\npadded_array = np.zeros((n_samples, n_mels, padding_length), dtype=np.float32)\n\nfor i, array in enumerate(S_dB_Total):\n    array = array.cpu().numpy()\n    cur_len = array.shape[1]\n    padded_array[i, :, :cur_len] = array  # only fill up to cur_len\n\n# Final 3D array\ndata_2D = padded_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:07:03.709271Z","iopub.execute_input":"2025-05-13T13:07:03.709541Z","iopub.status.idle":"2025-05-13T13:07:07.029615Z","shell.execute_reply.started":"2025-05-13T13:07:03.709522Z","shell.execute_reply":"2025-05-13T13:07:07.029074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_2D.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:07:17.618085Z","iopub.execute_input":"2025-05-13T13:07:17.618619Z","iopub.status.idle":"2025-05-13T13:07:17.623015Z","shell.execute_reply.started":"2025-05-13T13:07:17.618598Z","shell.execute_reply":"2025-05-13T13:07:17.622257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Emotion.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:09:30.211062Z","iopub.execute_input":"2025-05-13T13:09:30.211635Z","iopub.status.idle":"2025-05-13T13:09:30.217686Z","shell.execute_reply.started":"2025-05-13T13:09:30.211614Z","shell.execute_reply":"2025-05-13T13:09:30.216891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#normalization\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Reshape the array to 2D\nreshaped_data = data_2D.reshape((-1, 1))\n\n# Initialize the MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\nnormalized_data = scaler.fit_transform(reshaped_data)\n\n# Reshape the normalized data back to the original shape\nnormalized_data = normalized_data.reshape(data_2D.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:33.495417Z","iopub.execute_input":"2025-05-13T13:11:33.496002Z","iopub.status.idle":"2025-05-13T13:11:38.043873Z","shell.execute_reply.started":"2025-05-13T13:11:33.495980Z","shell.execute_reply":"2025-05-13T13:11:38.043257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:45.126703Z","iopub.execute_input":"2025-05-13T13:11:45.127054Z","iopub.status.idle":"2025-05-13T13:11:45.132731Z","shell.execute_reply.started":"2025-05-13T13:11:45.127031Z","shell.execute_reply":"2025-05-13T13:11:45.131845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=np.expand_dims(normalized_data, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:50.745463Z","iopub.execute_input":"2025-05-13T13:11:50.746046Z","iopub.status.idle":"2025-05-13T13:11:50.749561Z","shell.execute_reply.started":"2025-05-13T13:11:50.746004Z","shell.execute_reply":"2025-05-13T13:11:50.748903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels=df.Emotion.replace({'neutral': 1, 'happy': 3, 'sad': 4, 'angry': 5, 'fear': 6, 'disgust': 7, 'surprise': 8}).to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:18:04.194797Z","iopub.execute_input":"2025-05-13T13:18:04.195503Z","iopub.status.idle":"2025-05-13T13:18:04.207068Z","shell.execute_reply.started":"2025-05-13T13:18:04.195481Z","shell.execute_reply":"2025-05-13T13:18:04.206307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:57.985980Z","iopub.execute_input":"2025-05-13T13:11:57.986699Z","iopub.status.idle":"2025-05-13T13:11:57.991256Z","shell.execute_reply.started":"2025-05-13T13:11:57.986672Z","shell.execute_reply":"2025-05-13T13:11:57.990645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:59.265892Z","iopub.execute_input":"2025-05-13T13:11:59.266144Z","iopub.status.idle":"2025-05-13T13:11:59.270746Z","shell.execute_reply.started":"2025-05-13T13:11:59.266126Z","shell.execute_reply":"2025-05-13T13:11:59.270042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save({\n    'data': torch.tensor(data_2D),  \n    'labels': torch.tensor(labels) \n}, 'dataset.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:18:20.047978Z","iopub.execute_input":"2025-05-13T13:18:20.048669Z","iopub.status.idle":"2025-05-13T13:18:28.002085Z","shell.execute_reply.started":"2025-05-13T13:18:20.048646Z","shell.execute_reply":"2025-05-13T13:18:28.001218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Split","metadata":{}},{"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\n# Perform the train-test split (70% train & validation, 30% test )\nX_trainAndVal, X_test, y_trainAndVal, y_test = train_test_split(data, labels, test_size=0.3, stratify=labels, random_state=42)\n\n# Perform the train-validation split (5% test, 95% train)\nX_train, X_val, y_train, y_val = train_test_split(X_trainAndVal, y_trainAndVal, test_size=0.05, stratify=y_trainAndVal, random_state=42)\n\n# Print the sizes of each split\nprint(\"Train set size:\", len(X_train))\nprint(\"Validation set size:\", len(X_val))\nprint(\"Test set size:\", len(X_test))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-13T12:38:12.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass AudioDataset(Dataset):\n    def __init__(self,audio,label):\n        self.audios=audio\n        self.labels=label\n\n    def __len__(self):\n        return len(self.audios)\n\n    def get_batch_imgs(self, idx):\n        # Fetch a batch of inputs\n        return self.audios[idx]\n    \n    def get_batch_labels(self, idx):\n    # Fetch a batch of inputs\n        return self.labels[idx]\n\n    \n    def __getitem__(self, index):\n        audios=self.get_batch_imgs(index)\n        labels=self.get_batch_labels(index)\n        return audios,labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = AudioDataset(X_train,y_train)\nval_dataset = AudioDataset(X_val, y_val)\ntest_dataset = AudioDataset(X_test, y_test)\n\n# Create a DataLoader from the dataset\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader=DataLoader(val_dataset, batch_size=8, shuffle=True)\ntest_dataloader=DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}